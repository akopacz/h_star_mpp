import numpy as np
from exact_policy.utils import TARGET_NOT_FOUND_TSTEP
from envs.env import reverse_dict

TARGET_NOT_FOUND_VOTE = -1
PREC = 1e-6


def get_time_until_arrival(dist_from_target):
    target_tsteps = (dist_from_target == 0).nonzero()[0]
    if target_tsteps.size > 0:
        return target_tsteps[0]
    else:
        return TARGET_NOT_FOUND_TSTEP


def get_all_switches_from_obs(obs):
    all_switches = {
        agent: {
            agent_path: {
                obs[agent][agent_path].position[tstep]: tstep
                for tstep in (obs[agent][agent_path].next_switch_dist == 0).nonzero()[0]
            }
            for agent_path in obs[agent]
        }
        for agent in obs
    }
    # append current positions of the agents to the switches-dictionary
    # this is needed for h-shaped policy to work for scenarios
    #   when one of the agent already passed a switch and there exists maximum one switch
    #   between the two agents
    for agent in all_switches:
        for agent_path in all_switches[agent]:
            all_switches[agent][agent_path][obs[agent]
                                            [agent_path].position[0]] = 0

    return all_switches


def _get_action_to_move(coords1, coords2):
    return reverse_dict.get((coords2[0]-coords1[0], coords2[1]-coords1[1]), 4)


def get_actions_for_paths(obs):
    return {
        agent: {
            agent_path: int(
                _get_action_to_move(
                    obs[agent][agent_path].predicted_path[0],
                    obs[agent][agent_path].predicted_path[1]
                )
            )  # action needed to arrive to the next cell
            for agent_path in obs[agent]
        }
        for agent in obs
    }


def average_action_dict(action_dict, nr_of_agents, vote_size, actions_offset=0):
    """
    Calculate votes for different action based on the possible path lengths
    for each agent

    Parameters
    ----------
    action_dict: Dict[Tuple[int, int], int]
        Estimated time to reach target for possible agent-action pairs.
    nr_of_agents : int
        number of agents
    vote_size : int
        number of votes (output size for each agent)
    actions_offset : int, optional
        offset to shift with the actions to correspond indexes in the output vote vector
        if 0, the actions denote the indexes in the vote vector (for each agent)

    Returns
    -------
    _actions: Dict
        Dictionary with a vote vector of size vote_size,
        values in [0,1] interval or equal to TARGET_NOT_REACHED
        for each agent
    """
    
    # convert action dict to matrix format
    action_matrix = action_dict_to_matrix(action_dict, nr_of_agents, vote_size, actions_offset)

    action_matrix = normalize_votes(action_matrix, masked_inds= (action_matrix==TARGET_NOT_FOUND_VOTE))

    # return generated matrix as a dict
    return {
        agent: action_matrix[agent]
        for agent in range(nr_of_agents)
    }

def action_dict_to_matrix(action_dict, nr_of_agents, vote_size, actions_offset=0):
    """
    Convert the provided dictionary with agent actions to matrix format
    Shift each action if actions_offset is not 0  
    """
    action_matrix = np.empty((nr_of_agents, vote_size))
    action_matrix[:] = TARGET_NOT_FOUND_VOTE
    # count votes generated by exact policy
    for agent, action in action_dict:
        if action_dict[agent, action] != TARGET_NOT_FOUND_TSTEP:
            # votes <- nr of timesteps the exact policy estimated reaching the target cell
            #    for the current action
            action_matrix[agent, action +
                                actions_offset] = action_dict[agent, action]
    return action_matrix

def normalize_votes(action_matrix, masked_inds=None):
    """
    average votes by agents
    those elements that are on masked_inds position, remain unchanged  
    """
    if masked_inds is not None:
        if masked_inds.size == 0:
            masked_inds = None
        elif masked_inds.all():
            return masked_inds
        else:
            original_vals = action_matrix[masked_inds]
            # mask values during maximization
            action_matrix[masked_inds] = action_matrix[~ masked_inds].min() - 1

    action_matrix = action_matrix.max(axis=1).reshape(-1,1) - action_matrix

    if masked_inds is not None:
        # placeholder for sum
        action_matrix[masked_inds] = 0
    # average with sum
    _sum = action_matrix.sum(axis=1).reshape(-1, 1)
    _sum[_sum == 0] = 1
    action_matrix /= _sum

    if masked_inds is not None:
        action_matrix[masked_inds] = original_vals

    return action_matrix